% njr.tex
\documentclass[12pt]{article}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{graphicx} % support the \includegraphics command and optio
\usepackage[fleqn]{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{authblk}

\newcommand{\Var}{\mathop{\rm{Var}}}
\newcommand{\Cov}{\mathop{\rm{Cov}}}
\theoremstyle{definition}
\newtheorem*{example}{}

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

\title{A General Formula for Option Pricing}
\author[1]{Peter Carr}
\author[2]{Keith A. Lewis}
\affil[1]{NYU Tandon School of Engineering}
\affil[2]{KALX, LLC}
\date{\today}

\begin{document}
\maketitle
\begin{abstract}
One of the many nice features of the Black Merton Scholes (BMS)
model is that it generates a simple formula for European option
values. Unfortunately, the model typically contradicts observations
whenever two or more co-terminal option prices are observed. This
is because the model has only one free  parameter sigma which
determines both the mean and variance of the normally distributed log
price. By eschewing normality, one can be consistent with any observed
arbitrage-free volatility smile. In this paper, we present a new general
explicit formula for pricing European options in terms of the cumulants
of normalized log price.


\end{abstract}

\section{Outline}
Consider the problem of determining an arbitrage-free price of 
a European put written on a single risky asset.
%Suppose that the put and a zero coupon bond both mature at a fixed future date $t$.
The first Fundamental Theorem of Asset Pricing 
(FTAP) implies the existence of an equivalent martingale measure 
$\mathbb{Q}$, which corresponds to the choice of the money market account as numeraire.
Assuming that the final price $F$  of the 
underlying risky asset is strictly positive, 
there exists an equivalent probability measure $\mathbb{Q}^*$,
which corresponds to the choice of the put's underlying as numeraire.
We refer to $\mathbb{Q}$ and $\mathbb{Q}^*$ as risk-neutral measure and share measure respectively. 
Let $E$ and $E^*$ respectively denote expectations under the two measures.

The forward value of a $t$ maturity European put option having strike \(k > 0\) is
\begin{equation}
E \max\{k - F, 0\} = E(k - F)1(F \le k) = k \mathbb{Q} (F\le k) - f \mathbb{Q}^* (F\le k),
\label{afp}
\end{equation}
where $f = E F$ is the futures price of the underlying risky asset,  assuming continuous marking to market. 
The share measure $\mathbb{Q}^*$ is related to the risk-neutral measure 
$\mathbb{Q}$
by $d\mathbb{Q}^*/d\mathbb{Q} = F/f$,
which is the {\em Esscher transform}\cite{Ess1932} of the risk-neutral
measure $\mathbb{Q}$ evaluated at one. In this paper, 
we derive explicit series representations of the two probabilities of
the put finishing in-the-money,
\(\mathbb{Q}(F\le k)\) and \(\mathbb{Q}^*(F\le k)\).
The truncated version of these series representations will be accurate 
when the log price relative  $L \equiv \log \left( \frac{F}{f} \right)$ is approximately normal.

We suppose that $L$ has finite mean and standard deviation under both
measures \(\mathbb{Q}\) and \(\mathbb{Q}^*\). 
Let $s = \sqrt{E (L - EL)^2} > 0$ be the standard deviation of $L$ under
$\mathbb{Q}$.  Call $\kappa \equiv \log E e^{L - EL}$ the cumulant of $L$
under $\mathbb{Q}$.  The cumulant can be interpreted as the analog of
standard deviation when the exponential function replaces the quadratic
function as the convex function in Jensen's inequality. Option prices
can be interpreted as a strike-dependent reflection of the dispersion of
returns on their underlying asset, but this dispersion can be captured
equally well by $s$ or by $\kappa$.

Let $X \equiv \frac{L - EL}{s}$ be the normalized log price relative. 
By construction, $X$ has zero mean and unit standard deviation under  \(\mathbb{Q}\).
Clearly, $L - E L = sX$, so the cumulant 
can also be written as \(\kappa = \log E e^{sX} \equiv \kappa(s)\).
The function $\kappa(s)$ relating $\kappa$ to $s$ is called 
the {\em cumulant generating function} (CGF) of \(X\). 
Exponentiating both sides of $\kappa(s) \equiv \log E e^{sX}$
implies that \( E e^{sX -\kappa(s)} = 1 \) for all $s$.
By letting \(F = f e^{sX -\kappa(s)} \), we have \(EF = f\).
The quantity $f > 0$ can be observed as the unique strike at which 
European call and put values equate.
Clearly, \( L \equiv \log \left( \frac{F}{f} \right) =  sX -\kappa(s)  \).
Taking expectations under $\mathbb{Q}$, 
$EL = -\kappa(s)$.
%Taking expectations under $\mathbb{Q}^*$ instead, 
%$E^*L = s E^*X -\kappa(s) \equiv - \kappa^*s$.
 

We now show that the quantity $s>0$ can be obtained from the
out-of-the-money (OTM) put and call price profiles
$p(k), k \in (0,f)$ and $c(k), k \in [f,\infty)$.
First, calculate the forward price of the log contact 
$f_{\log} \equiv E  \log \left( \frac{F}{f} \right)$. From Carr-Madan\cite{CM2001}:
\[
	f_{\log} = \int_0^f \frac{p(k)}{k}\,\frac{dk}{k}
	+ \int_f^{\infty} \frac{c(k)}{k}\,\frac{dk}{k}.
\]
Second, calculate the forward price of the squared log contract 
$f_{\log^2} \equiv E  \left( \log^2 \left( \frac{F}{f} \right) \right)$.
\[
	f_{\log^2} = \int_0^f \frac{2}{k^2}
	  \left[ 1 - \log \left( \frac{k}{f} \right) \right] p(k)\,dk
          + \int_f^{\infty} \frac{2}{k^2}\left[ 1
	  - \log \left( \frac{k}{f} \right) \right] c(k)\,dk.
\]
Then $s = \sqrt{f_{\log^2} - f^2_{\log} } > 0$.

% \(m = \frac{ \log (k/f) + \kappa(s) }{s}\) 
 
Since \(F = f e^{sX -\kappa(s)} \),
the two probabilities of the put finishing in-the-money,
\(\mathbb{Q}(F\le k)\) and \(\mathbb{Q}^*(F\le k)\)
can each be expressed as
$\mathbb{Q} (X\le m)$ and $\mathbb{Q}^* (X\le m)$,
where \(m = \frac{ \log (k/f) + \kappa(s) }{s}\) 
%and \(m^* = \frac{ \log (k/f) + \kappa^*(s) }{s}\) 
measures the put's {\em moneyness}.
%under each measure.
As a result, the formula (\ref{afp}) for the arbitrage-free put futures price can be expressed as
\[
p(k) = E \max\{k - F, 0\} = k \mathbb{Q} (X\le m) - f \mathbb{Q}^* (X \le m).
\]

If \(X\) is standard normal and \(s = \sigma\sqrt{t}\),
then this is the Black\cite{Bla1976} put formula. Recall
\(
E\exp(N) = \exp(E N + \Var(N)/2)
\) 
if \(N\) is
normal, and 
\(
E \exp(N) f(M) = E\exp(N) Ef(M + \Cov(N,M))
\)
if \(N\) and \(M\) are jointly normal. We have
\(\kappa(s) = \log E \exp(sX) = s^2/2 \) and
\(E^* f(X) = E \exp(sX-s^2/2) f(X)
= E f(X + s)\). In the usual notation,
\(m = -d_2 = (\log (k/f) + \sigma^2t/2)/\sigma\sqrt{t} \)
and \(\mathbb{Q}^*(X \le m) = \mathbb{Q} (X + \sigma\sqrt{t} \le -d_2)
= \mathbb{Q}(X \le -d_1)\) where \(d_1 = d_2 + \sigma\sqrt{t}\).

In section 2, we recall some basic facts about cumulants and their
relationship to moments involving Bell polynomials.  Section 3 uses an
Edgeworth expansion, and Hermite polynomials to get an explicit formula
for the cumulative distribution function of the perturbed distribution.

A formula for calculating the Esscher transform with unit argument is
developed in section 4 using the same technique as in the preceding
section.

Section 5 catalogs how to use the general formula to calculate
option prices for many known models: Merton's jump diffusion \cite{},
Carr-Madan-???? variance gamma \cite{}, Kou's double exponential jump
\cite{}.
The primary contribution of this paper is an efficient option
pricing formula for any model that is a perturbation of
Black-Scholes/Merton.

The paper concludes with some general remarks.

\section{Cumulants}

The {\em cumulant generating function} of the random variable \(X\)
is \(\kappa(s) = \log Ee^{sX}\).
When they exist, the {\em cumulants}, \((\kappa_n)\),
are defined by
\[
\kappa(s) = \sum_{n=1}^\infty \kappa_n \frac{s^n}{n!}
\]
Since the \(n\)-th derivative evaluated at \(0\)
satisfies \(\kappa^{(n)}(s)|_{s = 0} = \kappa_n\) 
it is easy to
work out that
\(\kappa_1 = EX\) and \(\kappa_2 = \Var(X)\). Higher order
cumulants are less intuitive but the third and fourth are
related to skew and kurtosis. [Insert explicit formulas here.]


The cumulants of a random variable plus a constant are the 
same except the first cumulant is increased by the constant.
More generally, the cumulants of the sum of two independent 
random variables are the sums of their cumulants.
They scale homogeneously, the \(n\)-th cumulant of a constant
times a random variable is
\(\kappa_n(cX) = c^n\kappa_n(X)\).

The exponential of the cumulant in terms of
powers of \(s\) is
\[
Ee^{sX} =  \exp(\sum_{n=1}^\infty \kappa_n \frac{s^n}{n!})
= \sum_{n=0}^\infty B_n(\kappa_1,\dots,\kappa_n) \frac{s^n}{n!}
\]
where \(B_n(\kappa_1,\dots,\kappa_n)\) is the \(n\)-th complete
Bell polynomial.
This is just a special case of the
Fa\`a di Bruno formula first proved by Louis Fran\c{c}ois Antoine
Arborgast in 1800\cite{Arb1800}.
Bell polynomials satisfy the recurrence \cite{Com1974} \(B_0 = 1\) and
\[
B_{n+1}(x_1,\dots,x_{n+1}) = \sum_{k=0}^n \binom{n}{k}
B_{n - k}(x_1,\dots, x_{n - k}) x_{k+1}.
\]

The first few Bell polynomials are $B_0 = 1$, $B_1(x_1) = x_1$,
$B_2(x_1, x_2) = x_1^2 + x2$,
$B_3(x_1, x_2, x_3) =
%= B2 x1 + 2 B1 x2 + B0 x3
%= x1^3 +  x2 x1 + 2 x1 x2 + x3
x_1^3 + 3x_1x_2 + x_3$,
$B_4(x_1, x_2, x_4) =
%= B3 x1 + 3 B2 x2 + 3 B1 x3 + B0 x4
%= (x1^3 + 3 x_1 + x3)x1 + 3 (x1^2 + x2) x2 + 3 x1 x3 + B0 x4
%= (x1^4 + 3 x_1^2x_2 + x1x3) + 3 (x1^2x2 + x2^2)  + 3 x1 x3 + x4
%= x1^4 + 6 x_1^2x_2 + 4x1x3 + 3x2^2  + x4
x1^4 + 6 x_1^2 x_2 + 4 x_1 x_3 + 3 x_2^2  + x4$.

\section{Edgeworth Expansion}
Let $\psi$ be the probability density of a random variable, $X$,
with expected value 0 and mean 1.
The the Fourier transform is

\begin{align*}
E e^{-iuX} &= e^{\sum_{n=1}^\infty \kappa_n (-iu)^n/n!}\\
           &= e^{-u^2/2} e^{\sum_{n=3}^\infty \kappa_n (-iu)^n/n!}\\
           &= e^{-u^2/2} \sum_{n=0}^\infty B_n(0, 0, \kappa_3,...,\kappa_n)(-iu)^n/n!.\\
	   &= e^{-u^2/2} (1 + \sum_{n=3}^\infty B_n(0, 0, \kappa_3,...,\kappa_n)(-iu)^n/n!.\\
\end{align*}

where \((\kappa_n)\) are the cumulants of \(X\).

The Fourier transform, \(\hat{f}(u) = Ee^{-iuX}\),
of \(f'\) is \(iu \hat f(u)\) so
the Fourier transform of the \(n\)-th derivative
\(f^{(n)}\) is \((iu)^n\hat f(u)\) hence
\begin{align*}
\hat{\psi}(u) &= \hat{\phi}(u)(1 + \sum_{n=3}^\infty B_n(0, 0,\kappa_3,...,\kappa_n) (-iu)^n/n!\\
	&= \hat{\phi}(u) + \sum_{n=3}^\infty (-1)^n B_n(0, 0, \kappa_3,...,\kappa_n)
	\widehat{\phi^{(n)}}(u)/n!\\
\end{align*}
where \(\phi(x) = e^{-x^2/2}\). Note \(\hat{\phi}(u) = e^{-u^2/2}\).
Taking inverse Fourier transforms and integrating both
sides yields
\begin{equation}
	\Psi(x) = \Phi(x) + \sum_{n=3}^\infty (-1)^n B_n(0,0,\kappa_3,...,\kappa_n) \Phi^{(n)}(x)/n!.
\end{equation}

\subsection{Hermite Polynomials}
The derivatives of the standard normal density 
can be computed using Hermite polynomials\cite{AbrSte1964}
pp. 793--801.
One definition is
\[
H_n(x) = (-1)^n e^{x^2/2}\frac{d^n}{dx^n}e^{-x^2/2}.
\]
They satisfy the recurrence \(H_0(x) = 1\), \(H_1(x) = x\) and
\[
H_{n+1}(x) = xH_n(x) - n H_{n-1}(x).
\]
Note some authors use \(He_n(x)\) instead of \(H_n(x)\).
This shows \(\phi^{(n)}(x) = (-1)^n\phi(x) H_n(x)\)
so \(\Phi^{(n)} = (-1)^{n-1} H_{n-1}(x)\) for \(n > 0\).

We now have an explicit formula for the cumulative
distibution function of \(X\):
\begin{equation}
\Psi(x) = \Phi(x) - \phi(x)\sum_{n=1}^\infty
B_n(\kappa_1,\dots,\kappa_n) H_{n-1}(x)/n!
\end{equation}

\section{The Esscher Transform}

Given any random variable, \(X\), and number \(s\), define
\(X^*\) by \(P(X^*\le x) = P^*(X\le x)\) where
\(dP^*/dP = e^{-\kappa(s) + sX}\).
The cumulant of \(X^*\) is \(\kappa^*(u) = \kappa(u + s) - \kappa(s)\),
where \(\kappa\) is the cumulant of \(X\).
This follows from \(E^* e^{uX} = Ee^{sX - \kappa(s)} e^{uX}
= e^{\kappa(s + u) - \kappa(s)}\).

It follows that the \(n\)-th derivative satisfies
\(\kappa^{*(n)}(u) = \kappa^{(n)}(u + s)\), for \(n > 0\),\
and setting $u=0$, \(\kappa^*_n = \kappa^{*(n)}(0) = \kappa^{(n)}(s)\).

The last expression can be expressed as \(\kappa^{(n)}(s) =
\sum_{k=0}^\infty \kappa_{n - k} s^k/k!\), but for many random variables,
we can use the closed form solution on the left-hand side instead of
the infinite sum.

%
%The Edgeworth series expands the exponential in a power series.
%\[\exp\bigl(\sum_{n=0}^\infty \Delta\kappa_n (iu)^n/n!\bigr)
%= \sum_{k=0}^\infty (\sum_{n=0}^\infty \Delta\kappa_n (iu)^n/n!)^k/k!\]
%

%\subsection{Computations}
%Explicit formulas the first few Bell polynomials:
%\begin{align*}
%B_0 &= 1\\
%B_1 &= x_1\\
%B_2 &= B_1 x_1 + B_0 x_2\\
%&= x_1^2 + x_2\\
%B_3 &= B_2 x_1 + 2B_1 x_2 + B_0 x_3\\
%&= x_1^3 + x_1 x_2 + 2x_1 x_2 + x_3\\
%&= x_1^3 + 3x_1 x_2 + x_3\\
%B_4 &= B_3 x_1 + 3 B_2 x_2 + 3 B_1 x_3 + B_0 x_4\\
%&= (x_1^3 + 3x_1 x_2 + x_3) x_1  + 3 (x_1^2 + x_2)x_2 + 3 x_1 x_3 + x_4\\
%&= x_1^4 + 6x_1^2 x_2 + 4 x_1 x_3 + 3x_2^2 + x_4\\
%\end{align*}
%Explicit formulas for the first few Hermite polynomials:
%\begin{align*}
%H_0 &= 1\\
%H_1 &= x\\
%H_2 &= x^2 - 1\\
%H_3 &= x^3 - x\\
%H_4 &= x^4 - 6x^2 + 3\\
%H_5 &= x^5 - 10x^3 + 15x\\
%H_6 &= x^6 - 15x^4 + 45x^2 - 15\\
%\end{align*}
%
%Assuming \(\kappa_1 = \kappa_2 = 0\) the first few terms of the Edgeworth expansion are:
%\begin{align*}
%G(x) &= \sum_{n=0}^\infty (-1)^n B_n F^{(n)}(x)/n!\\
%&= F(x) - B_1 F'(x) + B_2F''(x)/2 - B_3F^{(3)}(x)/6 + B_4F^{(4)}/24\\
%&= F(x) + (-\kappa_3 (x^2 - 1)/6 - \kappa_4 (x^3-x)/24)e^{-x^2/2}/\sqrt{2\pi}\\
%\end{align*}
%The distribution is unimodal if and only if the second derivative has exactly one root.
%\begin{align*}
%G''(x) &= \sum_{n=0}^\infty (-1)^n B_n F^{(n+1)}(x)/n!\\
%&= F^{(2)}(x) - B_1 F^{(3)}(x) + B_2F^{(4)}(x)/2 - B_3F^{(5)}(x)/6 + B_4F^{(6)}/24\\
%&= (-x - \kappa_3 (x^4 - 6x^2 + 3)/6
%	 - \kappa_4 (x^5 - 10x^3 + 15x)/24)e^{-x^2/2}/\sqrt{2\pi}\\
%\end{align*}
%
\subsection{Examples}
\begin{example}[Normal]
A normally distributed random variable, \(X\), has density function 
\(f(x) = e^{-(x - \mu)^2/2\sigma^2}/\sigma\sqrt{2\pi}\), \(-\infty<x<\infty\),
with mean \(\mu\) and variance \(\sigma^2\).
The cumulant is \(\kappa(s) = \mu s + \sigma^2s^2/2\) so
\(\kappa_1 = \mu\) and \(\kappa_2 = \sigma^2\) are the only non-zero
cumulants. 

Since \(\kappa^*(u) = \kappa(u + s) - \kappa(s) = (\mu + \sigma^2s)u + \sigma^2u/2\)
we see that \(\kappa^*_1 = \kappa_1 + \sigma^2\) and \(\kappa^*_2 = \kappa_2\). The
Esscher transform of a normally distributed random variable is normal
with mean \(\mu^* = \mu + \sigma^2\) and variance \(\sigma^{*2} = \sigma^2\).

If the cumulants of a random variable vanish after some some point, then it must
be normal\cite{Luk1970} (Theorem 7.3.5). \end{example}

\begin{example}[Gamma]
A gamma distributed random variable, \(X\), has density function
\(f(x) = x^{\alpha - 1} e^{-x/\beta}/\beta^\alpha\Gamma(\alpha)\), \(x > 0\),
with mean \(\alpha\beta\) and variance \(\alpha\beta^2\).
The cumulant is \(\kappa(s) = -\alpha\log(1 - \beta s)\) so
\(\kappa_n = \kappa^{(n)}(0) = (n-1)!\alpha\beta^n\) since
\(\kappa^{(n)}(s) = (n-1)!\alpha\beta^n/(1 - \beta s)^n\).

Since \(\kappa^*(u) = \kappa(u + s) - \kappa(s) = -\alpha\log (1 - \beta u/(1 - \beta s)\),
the Esscher transform of a gamma distributed random variable is gamma
with \(\alpha^* = \alpha\) and \(\beta^* = \beta/(1 - \beta s)\).

Note that an exponentially distributed random variable is the special case when 
\(\alpha = 1\).
\end{example}

\begin{example}[Poisson]
If \(X\) is Poisson
with mean \(\mu\) then \(\kappa(s) = \mu(e^s - 1)\) so
\(\kappa_n = \mu\) for all \(n\) and
\(\kappa_n^* = \mu e^s\).
\end{example}
%\begin{example}[Compound Poisson]
%If \(Y\) is Poisson with mean \(\mu\) and \(Z_j\) are
%independent and identically distributed, define
%\(X = \sum_{j=1}^{Y} Z_j\). The cumulants of \(X\)
%are \(\kappa_n = ?\).
%\end{example}
%\begin{align*}
%Ee^{uX} &= \sum_{k=0}^\infty Ee^{u(Z_1 + \cdots Z_k)}\mu^k/k!\,e^{-\mu}\\
%&= \sum_{k=0}^\infty (Ee^{u Z_1})^k\mu^k/k!\,e^{-\mu}\\
%&= \sum_{k=0}^\infty (Ee^{u Z_1}\mu)^k/k!\,e^{-\mu}\\
%&= e^{\mu Ee^{u Z_1}}\,e^{-\mu}\\
%&= e^{\mu (Ee^{u Z_1} - 1)}\\
%\end{align*}
%Define \(\lambda(u) = \log Ee^{uZ_1}\). Then
%\(\log E e^{uX} = \mu(e^{\lambda(u)} -1)\).

\begin{example}[VG]
The Variance Gamma model is the difference of independent Gamma distributions
so \(\kappa_n = (n-1)!(\alpha\beta^n - \alpha'\beta'^n)\).
In order for this to be a perturbation of a standard normal
distribution we need \(0 = \alpha\beta - \alpha'\beta'\)
and \(1 = \alpha\beta^2 - \alpha'\beta'^2\).
Using mean and standard deviation
\(\mu = \mu'\) and \(\sigma^2 = 1 + \sigma'^2\).
For convergence we need \(1 < \sigma^2 \lll \mu\).
\end{example}

\begin{example}[Compound Poisson]
Assume \(Y_i\) are independent and identically distributed. If \(N\) is
Poisson with parameter \(\lambda\) then \(X\) is {\em compound Poisson}
if \(X = \sum_{i=0}^N Y_i\).
The exponential of the cumulant of X is
\begin{align*}
Ee^{sX} &= Ee^{s\sum_{i=0}^N Y_i}\\
&= \sum_{k=0}^\infty \frac{\lambda^k}{k!}e^{-\lambda}(Ee^{Y_i})^k\\
&= \sum_{k=0}^\infty \frac{\lambda^k}{k!}e^{-\lambda}(e^{\kappa^Y(s)})^k\\
&= \sum_{k=0}^\infty \frac{(\lambda e^{\kappa^Y(s)})^k}{k!}e^{-\lambda}\\
&= e^{\lambda(e^{\kappa(s) - 1})}
\end{align*}
\end{example}

\begin{example}[Jump Diffusion]
Merton's \cite{Mer?} jump diffusion model assumes
\(X = \alpha Z + \beta\sum_{k=0}^N Y_k\) where
\(N\) is Poisson and \(Y_k\) are independent normal.
\end{example}

\begin{example}[Double Exponential]
\(f(y) = p\eta_1 e^{-\eta_1 y}1(y > 0) + (1 - p)\eta_2 e^{\eta_2 y}1(y < 0)\)

\begin{align*}
Ee^{sY} &= \int_0^\infty e^{sx} pe^{-\eta_1 y}\,dy
+ \int_{-\infty}^0 e^{sy} (1-p)e^{\eta_2 y}\,dy\\
&= \frac{p}{1-s/\eta_1} + \frac{1-p}{1 + s/\eta_2}
\end{align*}

\(\kappa^Y_n = n!(\frac{p}{\eta_1^n} + \frac{1-p}{(-\eta_2)^n})\)
\end{example}

%\begin{align*}
%Ee^{uX} &= \int_0^\infty e^{ux} x^{\alpha-1} e^{-\beta x}
%\frac{\beta^\alpha}{\Gamma(\alpha)}\,dx\\
%&= \int_0^\infty x^{\alpha-1} e^{-(\beta -u) x}
%\frac{\beta^\alpha}{\Gamma(\alpha)}\,dx\\
%&= \frac{\Gamma(\alpha)}{(\beta - u)^\alpha} \frac{\beta^\alpha}{\Gamma(\alpha)}\\
%&= (1 - u/\beta)^{-\alpha}\\
%\end{align*}
%so
%\(
%\log E e^{uX} = \alpha \sum_{n=1}^\infty (u/\beta)^n/n\).

%At the money pricing ???
%%F = fe^{-k(s) + s X}, x \le k(s)/s
%
%\begin{align*}
%&P(F\le f) - P^*(F\le f) \\
%&= P(Y\le \kappa(s)/s) - P^*(Y\le \kappa(s)/s) \\
%&= G(\kappa(s)/s) - G^*(\kappa(s)/s) \\
%&= \sum_{n=1}^\infty (-B_n(\Delta\kappa) + B_n(\Delta\kappa^*))
%    e^{-(\kappa(s)/s)^2/2} H_{n-1}(\kappa(s)/s)/sqrt{2\pi} \\
%& = \Phi(s/2) - \Phi(-s/2) \\
%\end{align*}

%\(\chi^2\) with \(r\) degrees of freedom \(\kappa_n = 2^{n-1}(n-1)!r\).


\subsection{L\'evy Processes}

%Let \(X = aN + bP + c\) where \(N\) is standard normal and \(P\)
%is Poisson with mean \(\mu\).

%\(EX = b\mu + c\) and \(\Var X = a^2 + b^2\). Taking
%\(b = \sqrt{1 - a^2}\)

Kolmogorov's precursor to the L\'evy-Khintchine theorem\cite{Kol1992}
states that if a random variable \(X\) is infinitely divisible
and has finite variance
there exists a number \(\gamma\) and a non-decreasing function
\(G\) defined on the real line such that
\[
\kappa(s) = \log Ee^{sX} = \gamma s + \int_{-\infty}^\infty K_s(x)\,dG(x),
\]
where \(K_s(x) = (e^{sx} - 1 - sx)/x^2 = \sum_{n=2}^\infty s^nx^{n-2}/n!\).
Note the first cumulant of \(X\) is \(\gamma\) and for \(n\ge 2\),
\(\kappa_n = \int_{-\infty}^\infty x^{n-2}\,dG(x)\).

The Hamburger moment problem[cite?] provides the answer to
what the allowable cumulants are: the Hankel matrix
\([\kappa_{i+j}]_{i,j\ge 2}\) must be positive definite.

Since \(K_s(0) = s^2/2\) is the cumulant of the standard normal
distribution and \(a^2K_s(a) + as\) is the cumulant of a
Poisson distribution having mean \(a\),
infinitely divisible random variables can be
approximated by a normal plus a linear combination of
independent Poisson distributions.

%The K-model takes \(\gamma = 0\) and \(G\) of the form
%\[
%G(x) =
%\begin{cases}
%a e^{x/\alpha} &x < 0\\
%1 - be^{-x/\beta} & x > 0\\
%\end{cases}
%\]
%Note \(G\) jumps by \(1 - a - b\) at the origin. If \(a = b = 0\)
%this reduces to a standard normal distribution.
%
%The cumulants are simple to compute: \(\kappa_1 = 0\), \(\kappa_2 = 1\),
%and \(\kappa_{n+2} = (a(-\alpha)^n + b\beta^n)n!\) for \(n > 1\).

\section{Greeks}

$F = fe^{sX - \kappa(s)}$.

$v = E g(F)$.

$dv/df = Eg'(F) e^{sX - \kappa(s)} = E^*g'(F)$.

$dv/ds = E g'(F) fe^{sX - \kappa(s)} (X - \kappa'(s))
       = fE^* g'(F) (X - E^*X)$

\section{Remarks}
The Gram-Charlier A series expands the quotients of cumulative
distribution functions \(G/F\) using Hermite polynomials, but does not
have asymptotic convergence, whereas the Edgeworth expansion involves
the quotient of characteristic functions \(\hat G/\hat F\) in terms of
cumulants and does have asymptotic convergence, ignoring some dainty
facts \cite{Pet1975}.

If \((X_t)\) is a L\'evy process then \(X_1\) is
infinitely divisible and \(\log Ee^{sX_t} = t\kappa(s)\).
A consequence is that the volatility smile at a single
maturity determines the entire volatility surface, a fact that
may indicate L\'evy processes are not appropriate for
modeling stock prices.

A software implementation in C++ is available at
\url{https://fmsgjr.codeplex.com} and Excel add-ins
at \url{https://xllgjr.codeplex.com} that require
\url{https://xll.codeplex.com}.

\bibliographystyle{amsplain}
\bibliography{njr}

\end{document}
